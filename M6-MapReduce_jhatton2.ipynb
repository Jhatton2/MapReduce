{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f81ad21-e849-49c9-a718-9d3a1269ea26",
   "metadata": {},
   "source": [
    "# Map Reduce with Spark - Individual Assignment.\n",
    "\n",
    "### The code below uses map reduce in spark in order to provide a word count. Your task is to copy and modify the code in order to generate a similar analysis on a book of your choice.\n",
    "\n",
    " * Start by picking a book and finding its txt file https://www.gutenberg.org/\n",
    " * Ensure the starter code works. This starter code finds and lists the _most frequent single words_ that appear in the book.\n",
    " * Create three different map reduce operations that:\n",
    " * (1) One task that finds and lists the _longest single words_ used in the book. For example, the word \"three\" would have a length of \"5\".\n",
    " * (2) Another task that finds and lists the _most frequent bigrams_. A bigram is a pair of words that appear next to each other. For example, the phrase \"one two three four\" will have the bigrams \"one two\" \"two three\" and \"three four\".\n",
    " * (3) Another task that finds and lists a _customized statistic_. Pick any other kind of text counting statistic that you want to use map reduce for. Explain it and then implement it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f793267-33b1-4989-b9e4-1df00e2c06ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"MapReduceOperations\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load your book text from Gutenberg\n",
    "text_file = sc.textFile(\"https://www.gutenberg.org/cache/epub/228/pg228.txt\")\n",
    "\n",
    "# Preprocessing: clean the text by removing non-alphanumeric characters\n",
    "def clean_text(line):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', line).lower().split()\n",
    "\n",
    "cleaned_text = text_file.flatMap(clean_text)\n",
    "\n",
    "# Task 1: Find the longest single words\n",
    "def find_longest_words(cleaned_text):\n",
    "    # Map each word to its length\n",
    "    word_lengths = cleaned_text.map(lambda word: (word, len(word)))\n",
    "    # Find the maximum length\n",
    "    max_length = word_lengths.map(lambda x: x[1]).reduce(lambda a, b: max(a, b))\n",
    "    # Filter words that have the maximum length\n",
    "    longest_words = word_lengths.filter(lambda x: x[1] == max_length).map(lambda x: x[0]).distinct().collect()\n",
    "    return longest_words\n",
    "\n",
    "# Task 2: Find the most frequent bigrams\n",
    "def find_most_frequent_bigrams(cleaned_text, top_n=10):\n",
    "    # Create bigrams by zipping the RDD with its shifted version\n",
    "    bigrams = cleaned_text.zipWithIndex().map(lambda x: (x[1], x[0])).join(\n",
    "        cleaned_text.zipWithIndex().map(lambda x: (x[1] - 1, x[0]))).values()\n",
    "    # Count the frequency of each bigram\n",
    "    bigram_counts = bigrams.map(lambda x: (x, 1)).reduceByKey(lambda a, b: a + b)\n",
    "    # Get the top N bigrams by frequency\n",
    "    return bigram_counts.takeOrdered(top_n, key=lambda x: -x[1])\n",
    "\n",
    "# Task 3: Custom Statistic - Find the average word length\n",
    "def average_word_length(cleaned_text):\n",
    "    # Map each word to its length\n",
    "    word_lengths = cleaned_text.map(lambda word: len(word))\n",
    "    # Calculate the total length and the number of words\n",
    "    total_length = word_lengths.reduce(lambda a, b: a + b)\n",
    "    word_count = cleaned_text.count()\n",
    "    # Calculate the average word length\n",
    "    average_length = total_length / word_count\n",
    "    return average_length\n",
    "\n",
    "# Execution\n",
    "longest_words = find_longest_words(cleaned_text)\n",
    "print(\"Longest Words:\", longest_words)\n",
    "\n",
    "most_frequent_bigrams = find_most_frequent_bigrams(cleaned_text)\n",
    "print(\"Most Frequent Bigrams:\")\n",
    "for bigram, count in most_frequent_bigrams:\n",
    "    print(f\"{bigram[0]} {bigram[1]}: {count}\")\n",
    "\n",
    "average_length = average_word_length(cleaned_text)\n",
    "print(f\"Average Word Length: {average_length}\")\n",
    "\n",
    "# Stop SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14c14c-1c49-4307-b531-ec124eda0e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Create a bar plot using seaborn\n",
    "plt.figure(figsize=(10, 20))  # Adjust width and height as needed\n",
    "sns.barplot(x='count', y='word', data=top_n_df, orient='h')\n",
    "plt.title(f'Top {top_n} Most Frequent Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f3ecd-e375-4c8a-9970-e73aad8f8c1d",
   "metadata": {},
   "source": [
    "## Task 1: Longest Single Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bb487-5772-423d-94b8-66c3663af230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6e9a0-8753-4bf7-9514-837ba956dc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97e0ba76-648d-4876-8573-e228f7574a09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 2: Most Frequent Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fb6736-3335-483c-b246-74e5b36e0843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a57159-7b2d-4c00-a8b7-9f44203d5c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e0f9d6e-6f04-4a04-bb2d-45ac30945228",
   "metadata": {},
   "source": [
    "## Task 3: Customized Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc10e051-a690-44fe-aa0e-2f74f494355d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ae12a-792f-44c1-8780-1ae57a11465c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9852c377-230d-4179-8ce8-157de35c2c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ababb-f724-42a3-9b53-1000f9ea58d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
